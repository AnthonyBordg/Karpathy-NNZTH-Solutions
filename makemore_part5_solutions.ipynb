{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mEF05xtbHwp"
      },
      "source": [
        "## makemore: part 5 (building a WaveNet)\n",
        "\n",
        "[DeepMind blog post from 2016](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lDFibCgbHwr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "htdQCnUPbhS9",
        "outputId": "0b91d726-a179-4a19-a43c-c14cf755f71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-16 14:34:42--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-09-16 14:34:42 (6.00 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylW9Ir3GbHws",
        "outputId": "0fc1dc79-e32f-4de1-8377-368ee2466259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPwi-_rEbHwt",
        "outputId": "f978cb4b-000f-4a93-9faf-49a8d5b02d5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8JdduA3bHwu"
      },
      "outputs": [],
      "source": [
        "# shuffle up the words\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcrfsUG_bHwu",
        "outputId": "3284cec9-2eaa-4a39-d912-b6094384cc42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 8]) torch.Size([182625])\n",
            "torch.Size([22655, 8]) torch.Size([22655])\n",
            "torch.Size([22866, 8]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aFDMks6bHwv",
        "outputId": "3f0736da-7b63-480c-d5cf-ce210699ebf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........ --> y\n",
            ".......y --> u\n",
            "......yu --> h\n",
            ".....yuh --> e\n",
            "....yuhe --> n\n",
            "...yuhen --> g\n",
            "..yuheng --> .\n",
            "........ --> d\n",
            ".......d --> i\n",
            "......di --> o\n",
            ".....dio --> n\n",
            "....dion --> d\n",
            "...diond --> r\n",
            "..diondr --> e\n",
            ".diondre --> .\n",
            "........ --> x\n",
            ".......x --> a\n",
            "......xa --> v\n",
            ".....xav --> i\n",
            "....xavi --> e\n"
          ]
        }
      ],
      "source": [
        "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
        "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-goFZmrabHww"
      },
      "outputs": [],
      "source": [
        "# Near copy paste of the layers we have developed in Part 3\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      if x.ndim == 2:\n",
        "        dim = 0\n",
        "      elif x.ndim == 3:\n",
        "        dim = (0,1)\n",
        "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
        "      xvar = x.var(dim, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Embedding:\n",
        "\n",
        "  def __init__(self, num_embeddings, embedding_dim):\n",
        "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
        "\n",
        "  def __call__(self, IX):\n",
        "    self.out = self.weight[IX]\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight]\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class FlattenConsecutive:\n",
        "\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    x = x.view(B, T//self.n, C*self.n)\n",
        "    if x.shape[1] == 1:\n",
        "      x = x.squeeze(1)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Sequential:\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    # get parameters of all layers and stretch them out into one list\n",
        "    return [p for layer in self.layers for p in layer.parameters()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v01gpFOSbHwx"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42); # seed rng for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHMDMtvpbHwx",
        "outputId": "b3a6d5d4-8e75-4eb5-972b-4161bb15a2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76579\n"
          ]
        }
      ],
      "source": [
        "# original network\n",
        "# n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "# n_hidden = 300 # the number of neurons in the hidden layer of the MLP\n",
        "# model = Sequential([\n",
        "#   Embedding(vocab_size, n_embd),\n",
        "#   FlattenConsecutive(8), Linear(n_embd * 8, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "#   Linear(n_hidden, vocab_size),\n",
        "# ])\n",
        "\n",
        "# hierarchical network\n",
        "n_embd = 24 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh9SEATabHwy",
        "outputId": "67e023c5-ecd6-4ebe-cdce-67c2e286d56a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3167\n",
            "  10000/ 200000: 2.0576\n",
            "  20000/ 200000: 2.0723\n",
            "  30000/ 200000: 2.5134\n",
            "  40000/ 200000: 2.1476\n",
            "  50000/ 200000: 1.7836\n",
            "  60000/ 200000: 2.2592\n",
            "  70000/ 200000: 1.9331\n",
            "  80000/ 200000: 1.6875\n",
            "  90000/ 200000: 2.0395\n",
            " 100000/ 200000: 1.7736\n",
            " 110000/ 200000: 1.9569\n",
            " 120000/ 200000: 1.7465\n",
            " 130000/ 200000: 1.8126\n",
            " 140000/ 200000: 1.7406\n",
            " 150000/ 200000: 1.7466\n",
            " 160000/ 200000: 1.8805\n",
            " 170000/ 200000: 1.6266\n",
            " 180000/ 200000: 1.6476\n",
            " 190000/ 200000: 1.8555\n"
          ]
        }
      ],
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(lossi).shape"
      ],
      "metadata": {
        "id": "e-k14N7I6pUM",
        "outputId": "9fdee372-3e0f-43c2-f3bc-50ce07182f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200000])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrOSTEFzbHwz",
        "outputId": "0caebe4f-39ff-48bb-88e4-d56d85960401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7bc54bef9300>]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehknU7xtbHwz"
      },
      "outputs": [],
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W1wFpKubHwz",
        "outputId": "bca364c8-da1b-4e25-80f5-f1e623572528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1.7690284252166748\n",
            "val 1.9936527013778687\n"
          ]
        }
      ],
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQdB3cZYbHw0"
      },
      "source": [
        "### performance log\n",
        "\n",
        "- original (3 character context + 200 hidden neurons, 12K params): train 2.058, val 2.105\n",
        "- context: 3 -> 8 (22K params): train 1.918, val 2.027\n",
        "- flat -> hierarchical (22K params): train 1.941, val 2.029\n",
        "- fix bug in batchnorm: train 1.912, val 2.022\n",
        "- scale up the network: n_embd 24, n_hidden 128 (76K params): train 1.769, val 1.993\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSE5ZEgebHw0",
        "outputId": "475ae61d-c12d-4503-87a5-9c551ce5cb26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aiyanah.\n",
            "giusopf.\n",
            "lorron.\n",
            "roger.\n",
            "rhyitte.\n",
            "christell.\n",
            "jedmccee.\n",
            "kelipson.\n",
            "briyah.\n",
            "sylot.\n",
            "zennica.\n",
            "mythan.\n",
            "daxphon.\n",
            "petrit.\n",
            "adalie.\n",
            "jeniyah.\n",
            "glatipe.\n",
            "manaswi.\n",
            "yeslee.\n",
            "stephania.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass the neural net\n",
        "      logits = model(torch.tensor([context]))\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      # sample from the distribution\n",
        "      ix = torch.multinomial(probs, num_samples=1).item()\n",
        "      # shift the context window and track the samples\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      # if we sample the special '.' token, break\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w054JOpbHw1"
      },
      "source": [
        "### Next time:\n",
        "Why convolutions? Brief preview/hint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBP4pbSnbHw1",
        "outputId": "988e221b-0e39-4546-e3a2-e61e89e74239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........ --> d\n",
            ".......d --> i\n",
            "......di --> o\n",
            ".....dio --> n\n",
            "....dion --> d\n",
            "...diond --> r\n",
            "..diondr --> e\n",
            ".diondre --> .\n"
          ]
        }
      ],
      "source": [
        "for x,y in zip(Xtr[7:15], Ytr[7:15]):\n",
        "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhUwWXX8bHw2",
        "outputId": "3e8440a8-0077-48b2-ee36-8ff223ec7b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# forward a single example:\n",
        "logits = model(Xtr[[7]])\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBDyYGUWbHw2",
        "outputId": "7e50a8ed-2fda-48d8-85d1-7311b4098284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# forward all of them\n",
        "logits = torch.zeros(8, 27)\n",
        "for i in range(8):\n",
        "  logits[i] = model(Xtr[[7+i]])\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCoo_TvZbHw3"
      },
      "outputs": [],
      "source": [
        "# convolution is a \"for loop\"\n",
        "# allows us to forward Linear layers efficiently over space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: improve on the 1.993 loss! How far can we improve a WaveNet on this data?"
      ],
      "metadata": {
        "id": "8QWNI5GVsudw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reinititialize our model\n",
        "torch.manual_seed(42); # seed rng for reproducibility"
      ],
      "metadata": {
        "id": "GPNwoTCWs50l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first hyperparameter we can try to tweak is the embedding dimension. If I have to guess, I would say that with an embedding dimension of 24 Andrej's model is probably slightly overfitting the training set.\n",
        "\n",
        "A commonly used heuristic for the embedding dimension is\n",
        "\n",
        "$$n_{embd} = 4 \\times (\\text{block size})^{3/4}.$$\n",
        "In the case block_size = 8, we have $n_{embd} \\approx 19$.   \n"
      ],
      "metadata": {
        "id": "of6vkYZczVZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hierarchical network\n",
        "n_embd = 19 # we decrease the dimensionality of the embedding from 24 to 19\n",
        "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOvR-WTRwa9u",
        "outputId": "28dad6cc-d85f-4490-91bc-3d0957cf9834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DohXthycwrDu",
        "outputId": "f24569ba-f488-43e1-b095-0dc296d21bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2835\n",
            "  10000/ 200000: 2.1022\n",
            "  20000/ 200000: 2.3509\n",
            "  30000/ 200000: 1.7907\n",
            "  40000/ 200000: 2.0288\n",
            "  50000/ 200000: 1.8183\n",
            "  60000/ 200000: 2.0842\n",
            "  70000/ 200000: 2.0140\n",
            "  80000/ 200000: 1.8886\n",
            "  90000/ 200000: 1.9383\n",
            " 100000/ 200000: 1.9967\n",
            " 110000/ 200000: 1.6650\n",
            " 120000/ 200000: 2.0099\n",
            " 130000/ 200000: 2.0199\n",
            " 140000/ 200000: 2.0001\n",
            " 150000/ 200000: 1.8297\n",
            " 160000/ 200000: 1.9089\n",
            " 170000/ 200000: 1.7049\n",
            " 180000/ 200000: 1.5212\n",
            " 190000/ 200000: 1.7631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "metadata": {
        "id": "LMDwDq2uxZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8bvrzth3zF-",
        "outputId": "9d02c000-ada1-454a-c1bf-77209a793375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1.7791019678115845\n",
            "val 1.98908531665802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We slightly improved Andrej's val loss (while our train loss is slightly higher, a good sign that Andrej's model with its embedding dim of 24 was slightly overfitting.)\n",
        "\n",
        "Another heuristics for the embedding dimension that takes into account both the vocabulary size and the size of the context is\n",
        "$$n_{embd} = \\sqrt{vocab\\_size} + \\alpha \\times \\text{block_size}$$\n",
        "where $\\alpha$ is a small constant, $\\alpha = 1-4$, that depends on character dependancies in the language.\n",
        "\n",
        "With vocab_size = 27, block_size = 8 and $\\alpha = 2$, we have $n_{embd} \\approx 21$.  "
      ],
      "metadata": {
        "id": "tcVtXCKB6tB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reinititialize our model\n",
        "torch.manual_seed(42); # seed rng for reproducibility"
      ],
      "metadata": {
        "id": "M-PPB-Le-QCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hierarchical network\n",
        "n_embd = 21 # we decrease the dimensionality of the embedding from 24 to 21\n",
        "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGSVBjyv-U5O",
        "outputId": "6334e769-af69-4746-ae30-0e284f7ce3f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DV5mqnk-ek-",
        "outputId": "d9c88cab-ee85-4a0b-aa61-4485d5b6ad9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2991\n",
            "  10000/ 200000: 1.9214\n",
            "  20000/ 200000: 2.2635\n",
            "  30000/ 200000: 2.1180\n",
            "  40000/ 200000: 1.5018\n",
            "  50000/ 200000: 1.8226\n",
            "  60000/ 200000: 1.8666\n",
            "  70000/ 200000: 2.0374\n",
            "  80000/ 200000: 1.8875\n",
            "  90000/ 200000: 2.1082\n",
            " 100000/ 200000: 1.8035\n",
            " 110000/ 200000: 1.9963\n",
            " 120000/ 200000: 1.9101\n",
            " 130000/ 200000: 1.4987\n",
            " 140000/ 200000: 1.9396\n",
            " 150000/ 200000: 1.7501\n",
            " 160000/ 200000: 1.9970\n",
            " 170000/ 200000: 1.6498\n",
            " 180000/ 200000: 2.0101\n",
            " 190000/ 200000: 1.7819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "metadata": {
        "id": "XaUc8lUo-lcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjcQlPGtBiv-",
        "outputId": "d803d953-2d63-48c3-a76d-ae1ceffca9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1.7681214809417725\n",
            "val 1.9926327466964722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not as good as before, so let's try to decrease the number of dimensions of the embedding further, e.g. 16."
      ],
      "metadata": {
        "id": "PvrSQDpPCJzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reinititialize our model\n",
        "torch.manual_seed(42); # seed rng for reproducibility"
      ],
      "metadata": {
        "id": "AqOyTJ_8BmX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hierarchical network\n",
        "n_embd = 16 # we go as low as 16\n",
        "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_swbrWoCb4e",
        "outputId": "1d9165a8-86bc-49dc-f331-97f1104c88d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp14AvB3CfGO",
        "outputId": "f70735e4-4f1b-4442-b1f6-2d3e819ab807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3125\n",
            "  10000/ 200000: 2.2765\n",
            "  20000/ 200000: 1.8855\n",
            "  30000/ 200000: 1.8692\n",
            "  40000/ 200000: 2.1736\n",
            "  50000/ 200000: 2.3412\n",
            "  60000/ 200000: 2.2102\n",
            "  70000/ 200000: 1.8002\n",
            "  80000/ 200000: 1.7949\n",
            "  90000/ 200000: 1.9191\n",
            " 100000/ 200000: 1.8578\n",
            " 110000/ 200000: 1.9655\n",
            " 120000/ 200000: 2.1312\n",
            " 130000/ 200000: 1.8189\n",
            " 140000/ 200000: 1.4766\n",
            " 150000/ 200000: 1.7278\n",
            " 160000/ 200000: 1.7449\n",
            " 170000/ 200000: 2.1982\n",
            " 180000/ 200000: 1.6198\n",
            " 190000/ 200000: 1.5844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "metadata": {
        "id": "xTbxisYfCpg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5fIQq04Ffle",
        "outputId": "1bfc247f-3528-4700-ccbc-e41bc868a3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1.7768275737762451\n",
            "val 1.9921311140060425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not as good as previously with our 19-dim embedding. Can we tweak another hyperparamter, for instance the batch size? Let's increase the batch size to 64, in the hope that our model could converge faster."
      ],
      "metadata": {
        "id": "X2tschXDF3SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reinititialize our model\n",
        "torch.manual_seed(42); # seed rng for reproducibility"
      ],
      "metadata": {
        "id": "eGzDDSB0Fjpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hierarchical network\n",
        "n_embd = 19 # back to 19 dimensions\n",
        "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da2ue3QaGQPe",
        "outputId": "d4fc9c46-ad6f-49e7-9e36-34b6894ab7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 64 # we increase the batch size to 64\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpTwvlf-GdfO",
        "outputId": "6f4ce31f-1206-4ff4-a75f-136380023614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2918\n",
            "  10000/ 200000: 2.3934\n",
            "  20000/ 200000: 2.1227\n",
            "  30000/ 200000: 1.9520\n",
            "  40000/ 200000: 1.8085\n",
            "  50000/ 200000: 1.8804\n",
            "  60000/ 200000: 2.0030\n",
            "  70000/ 200000: 1.9237\n",
            "  80000/ 200000: 1.8494\n",
            "  90000/ 200000: 1.7907\n",
            " 100000/ 200000: 1.9030\n",
            " 110000/ 200000: 1.6300\n",
            " 120000/ 200000: 1.8036\n",
            " 130000/ 200000: 1.7313\n",
            " 140000/ 200000: 1.7654\n",
            " 150000/ 200000: 1.7836\n",
            " 160000/ 200000: 1.7300\n",
            " 170000/ 200000: 1.8196\n",
            " 180000/ 200000: 1.5590\n",
            " 190000/ 200000: 1.7296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "metadata": {
        "id": "KPq6s0flGq9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chy3DLzOKjxu",
        "outputId": "9d9f0215-0ee3-4a27-fc7d-5d06ff7ef366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1.6784805059432983\n",
            "val 2.0373897552490234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not as good as our best val loss! On the contrary, we should try to decrease the batch size to introduce more noise in the hope of our model achieving a better generalization.  "
      ],
      "metadata": {
        "id": "wJkFCzWlKvKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reinititialize our model\n",
        "torch.manual_seed(42); # seed rng for reproducibility"
      ],
      "metadata": {
        "id": "wMAcA4YRKoNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hierarchical network\n",
        "n_embd = 19 # back to 19 dimensions\n",
        "n_hidden = 128 # the number of neurons in the hidden layer of the MLP\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw55EFUPLQKR",
        "outputId": "bc9604fd-267d-4bf6-d09c-5c13f42a2a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 16 # more noise by decreasing the batch size\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19xBmALTLU_P",
        "outputId": "ff392da2-f330-4266-d66f-35dea2d7062b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2999\n",
            "  10000/ 200000: 2.5528\n",
            "  20000/ 200000: 2.1261\n",
            "  30000/ 200000: 2.0637\n",
            "  40000/ 200000: 2.2451\n",
            "  50000/ 200000: 2.0236\n",
            "  60000/ 200000: 1.8845\n",
            "  70000/ 200000: 1.6355\n",
            "  80000/ 200000: 2.0609\n",
            "  90000/ 200000: 1.7920\n",
            " 100000/ 200000: 2.1370\n",
            " 110000/ 200000: 2.9525\n",
            " 120000/ 200000: 2.0801\n",
            " 130000/ 200000: 1.5038\n",
            " 140000/ 200000: 2.1206\n",
            " 150000/ 200000: 2.3486\n",
            " 160000/ 200000: 2.0919\n",
            " 170000/ 200000: 1.6933\n",
            " 180000/ 200000: 2.1023\n",
            " 190000/ 200000: 2.2336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "metadata": {
        "id": "jYyLc-l_LfWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA8fmYB-N3Yl",
        "outputId": "5d636977-bf4e-4df1-8a52-8a16540ee2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1.8828433752059937\n",
            "val 1.997522234916687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slightly higher than our best val loss, so it seems that a batch size of 32 was a fair sweetspot.\n",
        "\n",
        "Last, we will try to tweak `n_hidden` the number of neurons in our hidden layers.  "
      ],
      "metadata": {
        "id": "VabQ200COFyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reinititialize our model\n",
        "torch.manual_seed(42); # seed rng for reproducibility"
      ],
      "metadata": {
        "id": "Rcg472jcN7gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hierarchical network\n",
        "n_embd = 19\n",
        "n_hidden = 150 # we slightly increase the number of neurons in our hidden layers, but not too much to avoid overfitting\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = model.parameters()\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUMlaiJEOzh4",
        "outputId": "b6103ba6-96df-4188-8d58-82888d18a49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32 # back to batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "id": "6QsdccP3O5rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6cce17-551d-4058-affd-3376c43a2cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3022\n",
            "  10000/ 200000: 2.4188\n",
            "  20000/ 200000: 2.0401\n",
            "  30000/ 200000: 2.2589\n",
            "  40000/ 200000: 2.2344\n",
            "  50000/ 200000: 2.1464\n",
            "  60000/ 200000: 2.0475\n",
            "  70000/ 200000: 2.3658\n",
            "  80000/ 200000: 1.8953\n",
            "  90000/ 200000: 1.7161\n",
            " 100000/ 200000: 1.8975\n",
            " 110000/ 200000: 1.8324\n",
            " 120000/ 200000: 1.8347\n",
            " 130000/ 200000: 1.7007\n",
            " 140000/ 200000: 2.0947\n",
            " 150000/ 200000: 1.5776\n",
            " 160000/ 200000: 1.5796\n",
            " 170000/ 200000: 1.6812\n",
            " 180000/ 200000: 1.9076\n",
            " 190000/ 200000: 1.7003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "metadata": {
        "id": "JmpILZ9tRG1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toZNgD7XTZAZ",
        "outputId": "7f7cadbb-039d-4a14-f73a-10b96aa79be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 1.7522053718566895\n",
            "val 1.990539312362671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We're probably starting to overfit"
      ],
      "metadata": {
        "id": "-tC4FOBofCnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our best validation loss: 1.989\n",
        "\n",
        "achieved with the following hyperparameters setup:\n",
        "- n_embd = 19\n",
        "- batch_size = 32\n",
        "- n_hidden = 128   "
      ],
      "metadata": {
        "id": "PEEPADuKiKpb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEKKSkHLi0cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}